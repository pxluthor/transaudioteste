import os
import streamlit as st
import google.generativeai as genai
import speech_recognition as sr
import toml
from pathlib import Path
import pyaudio
from st_audiorec import st_audiorec
import streamlit.components.v1 as components



config = toml.load("config.toml")


# Carregar a chave de API diretamente do arquivo de configura√ß√£o
api_key = st.secrets['api_keys']['google'] 
genai.configure(api_key=api_key)

# Configura√ß√£o da API do Google (Gemini)
generation_config = {
  "temperature": 0.3,
  "top_p": 0.95,
  "top_k": 64,
  "max_output_tokens": 8192,
  "response_mime_type": "text/plain",
}

model = genai.GenerativeModel(model_name = 'models/gemini-1.5-flash-latest',
                              generation_config=generation_config,)

# Fun√ß√£o para transcrever √°udio
def transcrever_audio(arquivo_audio):
    try:
        r = sr.Recognizer()
        with sr.AudioFile(arquivo_audio) as fonte:
            audio = r.record(fonte)
        texto = r.recognize_google(audio, language='pt-BR')  # Definindo o idioma para portugu√™s do Brasil
        return texto
    except sr.UnknownValueError:
        return "Erro: N√£o foi poss√≠vel entender o √°udio."
    except sr.RequestError as e:
        return f"Erro na transcri√ß√£o do √°udio: {e}"

# Fun√ß√£o para converter o papel de parede
def role_to_streamlit(role):
    return "assistente" if role == "model" else role

rec = sr.Recognizer()
#wav_audio_data = st_audiorec()
with st.sidebar:
  st.title("Grava√ß√£o de √Åudio")
  wav_audio_data = st_audiorec()
  
  st.divider()





#microfones = sr.Microphone().list_microphone_names()
#st.write(microfones)
#selected_microfones = st.multiselect("Selecione o(s) microfone(s)", microfones)

# Fun√ß√£o principal
def main():
    components.iframe("https://typebot_view.pxluthor.com.br/api/v1/typebots/meu-typebot-e9lpn27", height=600)
    # T√≠tulo
    st.title("üí¨ Chat - Transcription audio üéôüîâ")

   

    # Sidebar
    st.sidebar.button("Limpar Chat", on_click=limpar_chat)
    

    # Inicializa√ß√£o do chat
    if "chat" not in st.session_state:
        st.session_state.chat = model.start_chat(history=[])

    # Exibir hist√≥rico do chat
    for message in st.session_state.chat.history:
        with st.chat_message(role_to_streamlit(message.role)):
            st.markdown(message.parts[0].text)

    # Op√ß√µes de entrada: texto ou √°udio
    opcao_entrada = st.sidebar.radio("Selecione o tipo de entrada:", ("Texto", "√Åudio"))

    if opcao_entrada == "Texto":
        # Entrada de texto
        if st.button('Fale comigo'):
            #for microfone in selected_microfones:
            
            
            with sr.Microphone() as mic:
                rec.adjust_for_ambient_noise(mic)
                st.markdown("Pode falar que eu vou gravar")
                voz = rec.listen(mic)
                try:
                    texto = rec.recognize_google(voz, language="pt-BR")
                    prompt = texto
                    resp = model.generate_content(prompt)
                    st.chat_message("user").markdown(prompt)
                    with st.chat_message("assistente"): #mostra o √≠cone do assistente na tela
                        st.markdown(resp.text) # conte√∫do da resposta
            
                        
                except sr.UnknownValueError:
                    st.markdown("N√£o entendi o que voc√™ disse.")
                except sr.RequestError as e:
                    st.markdown(f"Erro ao solicitar o reconhecimento de voz: {e}")

        
        if prompt := st.chat_input("Como posso ajudar?",): # recebe um comando via chat_input e armazena no prompt.
            st.chat_message("user").markdown(prompt) # mostra a mensagem na tela (chat_message) - user formatando o prompt em markdown
            response = st.session_state.chat.send_message(prompt) # envia a mensagem ao assistente (chat.send_message) e armazena a resposta na vari√°vel response
            with st.chat_message("assistente"): #mostra o √≠cone do assistente na tela
                st.markdown(response.text) # conte√∫do da resposta
            
    else:
        # Entrada de √°udio
        arquivo_carregado = st.file_uploader("Carregar arquivo de √°udio (MP3 ou WAV)")
        st.chat_input("Como posso ajudar?")
        if arquivo_carregado:
            with open("audio_temp.mp3", "wb") as f:
                f.write(arquivo_carregado.read())
        # Enviar o arquivo local para a API
            your_file = genai.upload_file("audio_temp.mp3")
            st.info("Audio carregado !")
            
            prompt =''' voc√™ √© um analista de relacionamento em um callcenter da empresa Leste Telecom,
                        fa√ßa a transcri√ß√£o fiel ao √°udio, separando na apresesntando do resultado  as falas do cliente e atendente segundo o modelo de exemplo:
                        tempo - Atendente ou Cliente: Contexto da trancri√ß√£o de acordo com que est√° falando
                        
                        Fa√ßa um resumo do atendimento.
                        
                        informe os Pontos chave:
                        informe Poss√≠veis problemas:
                        informe Sugest√µes:
                        informe a Conclus√£o: '''
  





            
            resp = model.generate_content([prompt, your_file])
            

            if st.button("Fazer transcri√ß√£o"):
                
                response = st.session_state.chat.send_message(resp.text)
                with st.chat_message("Assistente"):
                    st.success('Transcri√ß√£o realizada')
                    st.markdown(resp.text)
                    #st.markdown(response.text)
           

# Fun√ß√£o para limpar o chat
def limpar_chat():
    st.session_state.chat.history.clear()
    #os.remove("audio_temp.mp3")




if __name__ == "__main__":
    main()
